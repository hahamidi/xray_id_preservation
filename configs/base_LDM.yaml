model:
  base_learning_rate: 5.0e-05
  scheduler_config:
      target: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
      params:
        num_train_timesteps: 1000
        beta_start: 0.00085
        beta_end: 0.012
        beta_schedule: scaled_linear
        prediction_type: epsilon
        clip_sample: false
        variance_type: fixed_small

  first_stage_config:
      target: diffusers.models.autoencoder_kl.AutoencoderKL
      params:
        act_fn: silu
        block_out_channels: [128, 256, 512, 512]
        down_block_types: [DownEncoderBlock2D, DownEncoderBlock2D, DownEncoderBlock2D, DownEncoderBlock2D]
        up_block_types:   [UpDecoderBlock2D,   UpDecoderBlock2D,   UpDecoderBlock2D,   UpDecoderBlock2D]
        latent_channels: 4
        in_channels: 3
        out_channels: 3
        scaling_factor: 0.18215

  unet_config:
      target: diffusers.models.unet_2d_condition.UNet2DConditionModel
      params:
        sample_size: 64
        in_channels: 4
        out_channels: 4
        down_block_types: [DownBlock2D, CrossAttnDownBlock2D, CrossAttnDownBlock2D, CrossAttnDownBlock2D]
        mid_block_type: UNetMidBlock2DCrossAttn
        up_block_types:   [UpBlock2D, CrossAttnUpBlock2D, CrossAttnUpBlock2D, CrossAttnUpBlock2D]
        block_out_channels: [320, 640, 1280, 1280]
        layers_per_block: 2
        cross_attention_dim: 2048
        attention_head_dim: [8, 8, 8, 8]
        center_input_sample: false
        flip_sin_to_cos: true
        freq_shift: 0
        downsample_padding: 1
        resnet_time_scale_shift: default
        use_linear_projection: false

  cond_stage_config:
      target: your_project.encoders.DynamicEncoder
      params:
        encode_key: text   # [text | label | image]
        model_name_or_path: null
        trainable: false

    # per-layer adapters stay here
  adapters_config:
      target: your_project.adapters.IPAdapters
      params:
        ip_scale_image: 1
        ip_scale_label: 1
        ip_num_tokens_image: 4
        ip_num_tokens_label: 4

        # image_projector:
        #   target: torch.nn.Linear
        #   params:
        #     in_features: 2048
        #     out_features: 2048

        # label_projector:
        #   target: torch.nn.Linear
        #   params:
        #     in_features: 2048
        #     out_features: 2048

  # ------------------------------------------------------------
  # Config for token conditioners (global encoders)
  # ------------------------------------------------------------
  model:
    token_conditioners:
      label_to_token:
        target: encoders.encoders.LabelToToken
        params:
          ip_num_tokens_label: 4
          in_features: 14
          out_features: 2048
          from_key: labels          # module will read batch["labels"]
      image_to_token:
        target: encoders.encoders.ImageToToken
        params:
          ip_num_tokens_image: 4
          in_features: 1024
          out_features: 2048
          from_key: image_embeds    # module will read batch["image_embeds"]  

    # Optional toggles (so you can freeze/train these nets)
  trainable:
      unet: true
      cond_stage: false
      adapters: true
      label_to_token: true          # NEW
      embed_to_token: true          # NEW

  weights:
      main_model: null
      vae: null
      adapters: null
      label_to_token: null          # NEW: path if you have a checkpoint
      image_to_token: null          # NEW

data:
  target: your_project.data.DatasetClass
  params:
    resolution: 1024
    # other paramters
